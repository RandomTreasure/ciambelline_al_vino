{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S-oUn2lD-5el"
      },
      "outputs": [],
      "source": [
        "import torch #pytorch\n",
        "from torchvision import datasets, transforms # importo da sets come MNIST e funzioni per trasformazioni di immagini\n",
        "from torch.utils.data import TensorDataset, DataLoader #datset e per caricare dati in batch\n",
        "import torch.nn as nn #classi per reti neurali\n",
        "import torch.nn.functional as F #relu, Cross entropy loss etc\n",
        "from sklearn.model_selection import train_test_split #split dei dati\n",
        "from sklearn.decomposition import PCA #PCA per riduzione dimensionale\n",
        "from sklearn.datasets import fetch_openml #scaricare i dati direttamente da OpenML\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm #color map\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "#K neighbors classifier e regressor\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn import neighbors\n",
        "\n",
        "#random forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#tsne\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lavoro con la GPU se possibile\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Computation device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIDn8V9s-6A5",
        "outputId": "58f3bbde-0246-4cff-e348-1d55b1da068b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computation device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot di figure (es. 2x2)\n",
        "fig,axs= plt.subplots(2, 2, figsize=(8, 8))\n",
        "\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.imshow(images[i], cmap='viridis')  # Usa una colormap a tua scelta\n",
        "    ax.set_title(f'Immagine {i+1}')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle(\"titolo del plot\")\n",
        "plt.tight_layout()  # Ottimizza lo spazio tra i subplot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1mcXs09YBAhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Istogramma\n",
        "plt.hist(data, bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Istogramma dei dati')\n",
        "plt.xlabel('Valori')\n",
        "plt.ylabel('Frequenza')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ilF6igj5BuWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_reshaped = images.reshape(-1, 28, 28) #reshape da (n dati, dimensione)-> (-1, dim1, dim2)\n",
        "\n",
        "# Suddivisione in dataset\n",
        "X_train, X_2, Y_train, Y_2=train_test_split(X_1, Y_1, train_size=7000, random_state=0) #7000 train 4000 resto\n",
        "X_val, X_test, Y_val, Y_test=train_test_split(X_2, Y_2, test_size=0.5, random_state=0) #2000 test 2000 validation\n",
        "\n",
        "\n",
        "#per cnn creo di dataloader\n",
        "# Conversione in tensori uso una funzione per alleggerire la riscrittura\n",
        "def to_tensor(x, y):\n",
        "    x_tensor=torch.tensor(x, dtype=torch.float32).reshape(-1, 1, 28, 28) #reshape per usare la CNN (evento, channel, h size, v size)\n",
        "    y_tensor=torch.tensor(y, dtype=torch.long) #etichette a 64 bit per interi\n",
        "    return TensorDataset(x_tensor, y_tensor)\n",
        "\n",
        "#creo i dataloader\n",
        "train_loader=DataLoader(to_tensor(X_train, Y_train), batch_size=64, shuffle=True)\n",
        "val_loader=DataLoader(to_tensor(X_val, Y_val), batch_size=256)\n",
        "test_loader= DataLoader(to_tensor(X_test, Y_test), batch_size=256)"
      ],
      "metadata": {
        "id": "3viLt9IADUu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#K Neighbors classifier\n",
        "model=neighbors.KNeighborsClassifier(n_neighbors=5)\n",
        "model.fit(train[features], train['Label']) #se non ho dati pandass usa train_features del tipo (n samples, n features)\n",
        "\n",
        "accuracy_train=model.score(train[features], train['Label'])\n",
        "accuracy_test=model.score(test[features], test['Label'])\n",
        "\n",
        "#K neighbors regressor\n",
        "model2= neighbors.KNeighborsRegressor(n_neighbors=5)\n",
        "model2.fit(train[features].values, train['Label'].values) #.values se sto usando dati pandas\n",
        "\n",
        "pred_train= model2.predict(train[features].values)\n",
        "pred_test=model2.predict(test[features].values)"
      ],
      "metadata": {
        "id": "Cdqnv_OeH9m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Crea il modello RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
        "#numero di alberi, profondità massima degli alberi, numero minimo di campioni richiesti per dividere un nodo, numero minimo di campioni richiesti per essere una foglia, per riproducibilià dei risultato\n",
        "model.fit(X_train, y_train) #alleno il modello\n",
        "y_pred = model.predict(X_test) #predizioni\n",
        "accuracy = accuracy_score(y_test, y_pred) #accuracy score\n",
        "\n",
        "#confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d')\n",
        "plt.title(\"Confusion Matrix - Random Forest\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "idlQtJ1fLHdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)  # riduciamo a 2 dimensioni\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Visualizzazione\n",
        "plt.figure(figsize=(10, 7))\n",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
        "plt.colorbar(scatter)\n",
        "plt.title(\"PCA - (2 componenti principali)\")\n",
        "plt.xlabel(\"PCA[0]\")\n",
        "plt.ylabel(\"PCA[1]\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dY5PvmUDNHpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tsne\n",
        "tsne=TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "X_tsne= tsne.fit_transform(X)\n",
        "\n",
        "# Visualizzazione\n",
        "plt.figure(figsize=(10, 7))\n",
        "scatter= plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
        "plt.colorbar(scatter)\n",
        "plt.title(\"t-SNE - Digits Dataset\")\n",
        "plt.xlabel(\"t-SNE 1\")\n",
        "plt.ylabel(\"t-SNE 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9-atK94YNuDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#definisco la CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1=nn.Conv2d(1, 32, kernel_size=3, padding=1) #28->14, filtro 3x3\n",
        "        self.pool=nn.MaxPool2d(2, 2)\n",
        "        self.conv2=nn.Conv2d(32, 64, kernel_size=3, padding=1) #14->7\n",
        "        self.fc1= nn.Linear(64*7*7, 64) #7->64\n",
        "        self.fc2= nn.Linear(64, 2)  # 64-> 2 neuroni per pari o dispari\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= F.relu(self.conv1(x))  # Conv → ReLU\n",
        "        x= self.pool(x)\n",
        "        x=F.relu(self.conv2(x))\n",
        "        x= self.pool(x)\n",
        "        x=x.view(-1, 64*7*7)\n",
        "        x=F.relu(self.fc1(x))\n",
        "        x=self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model= CNN()\n",
        "print(model)\n",
        "\n",
        "from torchsummary import summary\n",
        "if torch.cuda.is_available():\n",
        "  summary(model.cuda(), input_size=(1,28,28))\n",
        "else:\n",
        "  summary(model, input_size=(1,28,28))"
      ],
      "metadata": {
        "id": "oYkjedtIOOBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred, y_true):\n",
        "  y_pred= torch.argmax(y_pred, axis=1)\n",
        "  y_true= torch.argmax(y_true, axis=1)\n",
        "\n",
        "  correct=(y_pred==y_true).sum().item()\n",
        "\n",
        "  total= len(y_true)\n",
        "  acc= correct/ total\n",
        "  return acc"
      ],
      "metadata": {
        "id": "W-oZrDXCOj0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "num_epochs =120\n",
        "patience= 20 #per early stopping\n",
        "\n",
        "best_val_acc= 0.0 #salvo la miglior validation accuracy\n",
        "epochs_since_best_val_acc=0 #salvo la miglior epoch con la migliore accuracy\n",
        "\n",
        "train_curve=[]\n",
        "val_curve=[]\n",
        "lr_curve=[]\n",
        "pesi=[]\n",
        "\n",
        "# Train loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    tmp_loss=0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        output=model(data)\n",
        "        loss=criterion(output, target)\n",
        "        optimizer.zero_grad()# clear the previous gradients\n",
        "        loss.backward()# compute gradient of loss\n",
        "        optimizer.step()# update the weigths\n",
        "        tmp_loss += loss.detach().numpy()\n",
        "\n",
        "    if epoch>20:\n",
        "     scheduler.step()\n",
        "\n",
        "    pesi.append({parametro: tensore.clone().detach() for parametro, tensore in model.state_dict().items()}) #salvo i pesi\n",
        "    lr_curve.append(optimizer.param_groups[0]['lr'])\n",
        "    train_curve.append(tmp_loss/len(train_loader))\n",
        "\n",
        "    # Validation\n",
        "    model.eval() # the validation step do NOT change the parameters\n",
        "    with torch.no_grad():\n",
        "        val_acc=0.0\n",
        "        val_total= 0\n",
        "        val_loss=0\n",
        "        for data, target in val_loader:\n",
        "            output=model(data)\n",
        "            val_loss+=criterion(output, target).item()\n",
        "            val_acc+=accuracy(output, target)\n",
        "\n",
        "        val_acc/=len(val_loader)\n",
        "        val_loss/=len(val_loader)\n",
        "        val_curve.append(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}, lr: {optimizer.param_groups[0]['lr']:.3e}\")\n",
        "\n",
        "        # Check if the validation accuracy has improved\n",
        "        if val_acc>best_val_acc:\n",
        "            best_val_acc=val_acc\n",
        "            epochs_since_best_val_acc=0\n",
        "            best_weights=model.state_dict()\n",
        "            torch.save(best_weights, 'results/best_weights.pth')\n",
        "            print(\"Best!\")\n",
        "            best_epoch=epoch\n",
        "        else:\n",
        "            epochs_since_best_val_acc += 1\n",
        "\n",
        "        # Check if early stopping is necessary\n",
        "        if epochs_since_best_val_acc>=patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('BCE Loss')\n",
        "plt.legend()\n",
        "plt.title(f'Training and Validation Loss - {title}')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "U-AFPzFGO0J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "def train_model(model, train_loader, val_loader, epochs=20):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "  # Training loop con checkpoint\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  best_val_loss = float('inf')\n",
        "  best_model_path = \"best_model.pt\"\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for xb, yb in train_loader:\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          output = model(xb)\n",
        "          loss = criterion(output, yb)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          running_loss += loss.item() * xb.size(0)\n",
        "      train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "      model.eval()\n",
        "\n",
        "      with torch.no_grad():\n",
        "          val_loss = sum(criterion(model(xb.to(\"cuda\")), yb.to(\"cuda\")).item() * xb.size(0)\n",
        "                        for xb, yb in val_loader) / len(val_loader.dataset)\n",
        "\n",
        "      train_losses.append(train_loss)\n",
        "      val_losses.append(val_loss)\n",
        "\n",
        "      print(f\"Epoch {epoch+1:2d}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "      if val_loss < best_val_loss:\n",
        "          best_val_loss = val_loss\n",
        "          torch.save(model.state_dict(), best_model_path)\n",
        "          print(\"Model saved!\")\n",
        "\n",
        "  # Carica miglior modello\n",
        "  model.load_state_dict(torch.load(best_model_path))\n",
        "  return model, train_losses, val_losses"
      ],
      "metadata": {
        "id": "Ksbp4eDmR7mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training per 20 epoche e salvo i pesi usati\n",
        "pesi1 = []\n",
        "for epoch in range(20):\n",
        "  start_time = time.time()\n",
        "  model.train()\n",
        "  for batch in train_loader:\n",
        "      x, y = batch\n",
        "      optimizer.zero_grad()\n",
        "      output = model(x)\n",
        "      loss = criterion(output, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "  end_time = time.time()\n",
        "  elapsed = end_time - start_time\n",
        "  #salvo i pesi\n",
        "  pesi1.append({parametro: tensore.clone().detach() for parametro, tensore in model.state_dict().items()})\n",
        "  print(f\"Epoch {epoch+1} completata in {elapsed:.2f} secondi\")\n",
        "\n",
        "\n",
        "#Validation\n",
        "for epoch in range(20):\n",
        "    start_time = time.time()\n",
        "    # Fase di validazione\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss_q2=[]\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            X_val, Y_val = batch\n",
        "            output_val = model(X_val)\n",
        "            loss_val = criterion(output_val, Y_val)\n",
        "            val_loss += loss_val.item()\n",
        "            _, predicted = torch.max(output_val, 1)\n",
        "            correct += (predicted == Y_val).sum().item()\n",
        "            total += Y_val.size(0)\n",
        "\n",
        "    accuracy = correct / total * 100\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_loss_q2.append(avg_val_loss)\n",
        "\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed = end_time - start_time\n",
        "    print(f\"Epoch {epoch+1:02d} completata in {elapsed:.2f}s | Val Loss: {avg_val_loss:.4f} | Val Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "dKjton3CUBzG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}